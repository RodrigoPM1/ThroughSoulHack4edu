{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qmax_ZeXQBg"
      },
      "outputs": [],
      "source": [
        "# Instalar OpenCV\n",
        "!pip install opencv-python-headless\n",
        "\n",
        "# Instalar DeepFace\n",
        "!pip install deepface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "from deepface import DeepFace\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Función para capturar la foto desde la webcam\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "        const div = document.createElement('div');\n",
        "        const capture = document.createElement('button');\n",
        "        capture.textContent = 'Capture';\n",
        "        div.appendChild(capture);\n",
        "\n",
        "        const video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "        document.body.appendChild(div);\n",
        "        div.appendChild(video);\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        // Redimensiona la salida para ajustarse al elemento de video.\n",
        "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "        // Espera a que se haga clic en Capturar.\n",
        "        await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        stream.getVideoTracks()[0].stop();\n",
        "        div.remove();\n",
        "        return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename"
      ],
      "metadata": {
        "id": "gm280vfDXniV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# Inicializar el clasificador de rostros Haar\n",
        "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "try:\n",
        "    # Capturar la foto\n",
        "    filename = take_photo()\n",
        "    print('Saved to {}'.format(filename))\n",
        "\n",
        "    # Mostrar la imagen capturada\n",
        "    display(Image(filename))\n",
        "\n",
        "    # Leer la imagen capturada usando OpenCV\n",
        "    img = cv2.imread(filename)\n",
        "\n",
        "    if img is not None:\n",
        "        # Convertir a escala de grises para la detección de rostros\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Mostrar la imagen original\n",
        "        print(\"Imagen original:\")\n",
        "        cv2_imshow(img)\n",
        "\n",
        "        # Detectar rostros en la imagen\n",
        "        faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "        if len(faces) == 0:\n",
        "            print(\"No se detectaron rostros en la imagen.\")\n",
        "        else:\n",
        "            # Analizar cada rostro detectado\n",
        "            for (x, y, w, h) in faces:\n",
        "                rostro = img[y:y+h, x:x+w]\n",
        "\n",
        "                try:\n",
        "                    # Analizar el rostro para detectar emociones\n",
        "                    result = DeepFace.analyze(rostro, actions=['emotion'], enforce_detection=False)\n",
        "                    print(result)\n",
        "\n",
        "                    # Obtener la emoción dominante\n",
        "                    dominant_emotion = result[0]['dominant_emotion']\n",
        "                    print(f\"Emoción dominante: {dominant_emotion}\")\n",
        "\n",
        "                    # Dibujar un rectángulo alrededor del rostro y mostrar la emoción\n",
        "                    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "                    cv2.putText(img, dominant_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"No se pudo analizar el rostro: {e}\")\n",
        "\n",
        "            # Mostrar la imagen con las emociones detectadas\n",
        "            cv2_imshow(img)\n",
        "\n",
        "    else:\n",
        "        print(f\"No se pudo leer la imagen: {filename}\")\n",
        "\n",
        "except Exception as err:\n",
        "    print(f\"Error: {str(err)}\")"
      ],
      "metadata": {
        "id": "422MQLQuXza9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}